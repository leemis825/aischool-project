import { useNavigate } from "react-router-dom";
import Layout from "../components/Layout.js";
import { useEffect, useRef, useState } from "react";
// ì–¸ë‹ˆ ì—¬ê¸°ì˜ˆìš”1ğŸ¦ŠğŸ°
import { sttAndMinwon } from "../services/sttService";
import { requestTts } from "../services/ttsService";

export default function ListeningPage() {
  const navigate = useNavigate();

  const [isRecording, setIsRecording] = useState(false);
  const [isUploading, setIsUploading] = useState(false);
  const [error, setError] = useState<string | null>(null);
  const [sttResult, setSttResult] = useState<string>("");
  const [ttsUrl, setTtsUrl] = useState<string | null>(null); // ğŸ”¹ TTS ì˜¤ë””ì˜¤ URL

  // ğŸ”¹ ë…¹ìŒ ê´€ë ¨ refë“¤
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const mediaStreamRef = useRef<MediaStream | null>(null);
  const chunksRef = useRef<Blob[]>([]);

  // ğŸ”¹ ì˜¤ë””ì˜¤ ë¹„ì£¼ì–¼ë¼ì´ì €ìš© refë“¤
  const audioContextRef = useRef<AudioContext | null>(null);
  const analyserRef = useRef<AnalyserNode | null>(null);
  const animationFrameRef = useRef<number | null>(null);
  const canvasRef = useRef<HTMLCanvasElement | null>(null);

  useEffect(() => {
    const setupRecorderAndVisualizer = async () => {
      try {
        const stream = await navigator.mediaDevices.getUserMedia({
          audio: true,
        });
        mediaStreamRef.current = stream;

        /**
         * 1) MediaRecorder ì„¤ì • (ë…¹ìŒìš©)
         */
        const options: MediaRecorderOptions = {};
        if (MediaRecorder.isTypeSupported("audio/webm;codecs=opus")) {
          options.mimeType = "audio/webm;codecs=opus";
        } else if (MediaRecorder.isTypeSupported("audio/webm")) {
          options.mimeType = "audio/webm";
        }

        const recorder = new MediaRecorder(stream, options);

        recorder.ondataavailable = (event: BlobEvent) => {
          if (event.data && event.data.size > 0) {
            chunksRef.current.push(event.data);
          }
        };

        recorder.onstop = async () => {
          try {
            const blob = new Blob(chunksRef.current, { type: "audio/webm" });
            chunksRef.current = [];

            await uploadBlob(blob);
          } catch (err) {
            console.error(err);
            setError("ë…¹ìŒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆì–´ìš”. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.");
            setIsUploading(false);
          }
        };

        mediaRecorderRef.current = recorder;

        /**
         * 2) AudioContext + Analyser ì„¤ì • (íŒŒí˜• ê·¸ë¦¬ê¸°ìš©)
         */
        const audioCtx = new (window.AudioContext ||
          (window as any).webkitAudioContext)();
        audioContextRef.current = audioCtx;

        const source = audioCtx.createMediaStreamSource(stream);
        const analyser = audioCtx.createAnalyser();
        analyser.fftSize = 2048; // í•´ìƒë„
        source.connect(analyser);
        analyserRef.current = analyser;

        // íŒŒí˜• ê·¸ë¦¬ê¸° ì‹œì‘
        if (canvasRef.current && analyserRef.current) {
          drawWaveform();
        }

        // ìë™ ë…¹ìŒ ì‹œì‘
        recorder.start();
        setIsRecording(true);
      } catch (e) {
        console.error(e);
        setError("ë§ˆì´í¬ ê¶Œí•œì„ í—ˆìš©í•´ ì£¼ì„¸ìš”.");
      }
    };

    setupRecorderAndVisualizer();

    // ì–¸ë§ˆìš´íŠ¸ ì‹œ ì •ë¦¬
    return () => {
      try {
        if (
          mediaRecorderRef.current &&
          mediaRecorderRef.current.state !== "inactive"
        ) {
          mediaRecorderRef.current.stop();
        }
      } catch {
        // ignore
      }

      if (mediaStreamRef.current) {
        mediaStreamRef.current.getTracks().forEach((t) => t.stop());
      }

      if (animationFrameRef.current !== null) {
        cancelAnimationFrame(animationFrameRef.current);
      }

      if (audioContextRef.current) {
        audioContextRef.current.close();
      }
    };
    // eslint-disable-next-line react-hooks/exhaustive-deps
  }, []);

  // ğŸ”¹ ìº”ë²„ìŠ¤ì— ì‹¤ì‹œê°„ íŒŒí˜• ê·¸ë¦¬ëŠ” í•¨ìˆ˜
  const drawWaveform = () => {
    const canvas = canvasRef.current;
    const analyser = analyserRef.current;
    const audioCtx = audioContextRef.current;

    if (!canvas || !analyser || !audioCtx) return;

    const canvasCtx = canvas.getContext("2d");
    if (!canvasCtx) return;

    const bufferLength = analyser.fftSize;
    const dataArray = new Uint8Array(bufferLength);

    const draw = () => {
      animationFrameRef.current = requestAnimationFrame(draw);

      analyser.getByteTimeDomainData(dataArray);

      const { width, height } = canvas;
      canvasCtx.clearRect(0, 0, width, height);

      // ë°°ê²½
      canvasCtx.fillStyle = "#CBF3C7";
      canvasCtx.fillRect(0, 0, width, height);

      // íŒŒí˜• ìŠ¤íƒ€ì¼
      canvasCtx.lineWidth = 3;
      canvasCtx.strokeStyle = "#4E9948";

      canvasCtx.beginPath();

      const sliceWidth = (width * 1.0) / bufferLength;
      let x = 0;

      for (let i = 0; i < bufferLength; i++) {
        const value = dataArray[i] ?? 128; // ê¸°ë³¸ê°’ 128 â†’ ì¤‘ì•™ì„ 
        const v = value / 128.0;
        const y = (v * height) / 2;
        if (i === 0) {
          canvasCtx.moveTo(x, y);
        } else {
          canvasCtx.lineTo(x, y);
        }

        x += sliceWidth;
      }

      canvasCtx.lineTo(width, height / 2);
      canvasCtx.stroke();
    };

    draw();
  };

  const stopVisualizer = () => {
    // ì• ë‹ˆë©”ì´ì…˜ ë£¨í”„ ì¤‘ì§€
    if (animationFrameRef.current !== null) {
      cancelAnimationFrame(animationFrameRef.current);
      animationFrameRef.current = null;
    }

    // ìº”ë²„ìŠ¤ ê¹¨ë—í•˜ê²Œ ì§€ìš°ê³  ë°°ê²½ë§Œ ì±„ìš°ê¸°
    const canvas = canvasRef.current;
    if (!canvas) return;

    const ctx = canvas.getContext("2d");
    if (!ctx) return;

    const { width, height } = canvas;
    ctx.clearRect(0, 0, width, height);
    ctx.fillStyle = "#4E9948";
    ctx.fillRect(0, 0, width, height);
  };

  // ì–¸ë‹ˆ ì—¬ê¸°ì˜ˆìš”2ğŸ¦ŠğŸ°
  const callTTS = async (text: string) => {
    try {
      const trimmed = text?.trim();
      if (!trimmed) return;

      // ì´ì „ì— ë§Œë“  ì˜¤ë””ì˜¤ URLì´ ìˆìœ¼ë©´ ì •ë¦¬
      if (ttsUrl) {
        URL.revokeObjectURL(ttsUrl);
      }

      const blob = await requestTts(trimmed); // â† ì„œë¹„ìŠ¤ í•¨ìˆ˜ í˜¸ì¶œ
      const url = URL.createObjectURL(blob);
      setTtsUrl(url);
    } catch (e) {
      console.error(e);
      setError("ì•ˆë‚´ ìŒì„±ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆì–´ìš”.");
    }
  };
  // ğŸ”¹ Blobì„ ë°›ì•„ì„œ /stt ë¡œ ì—…ë¡œë“œ
  const uploadBlob = async (blob: Blob) => {
    setError(null);

    try {
      console.log("ğŸ¤ì „ì†¡í•  ì˜¤ë””ì˜¤ Blob:", blob);
      console.log("í¬ê¸°(bytes):", blob.size);
      console.log("íƒ€ì…:", blob.type);
      const file = new File([blob], "voice.webm", { type: "audio/webm" });
      console.log("ğŸ¤ìƒì„±ëœ File:", file);

      // ì–¸ë‹ˆ ì—¬ê¸°ì˜ˆìš”3ğŸ¦ŠğŸ°
      const resultText = await sttAndMinwon(file);
      console.log("ğŸ”Š STT+ë¯¼ì› ì—”ì§„ ê²°ê³¼:", resultText);

      const finalText = resultText || "(ë¹ˆ í…ìŠ¤íŠ¸)";
      setSttResult(finalText);
      setIsUploading(false);

      // ğŸ”¹ STT ê²°ê³¼ë¥¼ ìŒì„±ìœ¼ë¡œë„ ì•ˆë‚´
      await callTTS(
        finalText || "ë¯¼ì›ì´ ì ‘ìˆ˜ë˜ì—ˆìŠµë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ ì£¼ì„¸ìš”."
      );

      // ë‚˜ì¤‘ì— summary í˜ì´ì§€ë¡œ ì´ë™í•˜ë ¤ë©´ ì—¬ê¸°ì—ì„œ navigate
      // navigate("/summary", { state: { sttText: finalText } });
    } catch (e) {
      console.error(e);
      setError("ë…¹ìŒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆì–´ìš”. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.");
      setIsUploading(false);
    }
  };

  // ğŸ”¹ í™”ë©´ íƒ­ ì‹œ: ë…¹ìŒ ì¢…ë£Œ + ì—…ë¡œë“œ
  const handleClick = () => {
    if (!isRecording || isUploading) return;
    if (!mediaRecorderRef.current) {
      setError("ë…¹ìŒê¸°ê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ì–´ìš”.");
      return;
    }

    setIsUploading(true);
    navigate("/summary");

    try {
      mediaRecorderRef.current.stop();
      setIsRecording(false);

      // ğŸ”¥ ì—¬ê¸°ì„œ íŒŒí˜• ë„ê¸°
      stopVisualizer();
    } catch (e) {
      console.error(e);
      setError("ë…¹ìŒì„ ì¤‘ë‹¨í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆì–´ìš”.");
      setIsUploading(false);
    }
  };

  return (
    <Layout
      title="ë¯¼ì›ì ‘ìˆ˜"
      content="ë¯¼ì›ì„ ë“£ê³  ìˆì–´ìš”"
      topImage="src/assets/top2.png"
      onClick={handleClick}
    >
      <div
        style={{
          background: "#CBF3C7",
          padding: "16px 24px",
          borderRadius: "24px",
          fontSize: "20px",
          lineHeight: 1.4,
          textAlign: "center",
          width: "90%",
        }}
      >
        {/* íŒŒí˜• ìº”ë²„ìŠ¤ */}
        <canvas
          ref={canvasRef}
          width={600}
          height={120}
          style={{
            display: "block",
            margin: "0 auto 16px",
            borderRadius: "16px",
          }}
        />

        {error && <p style={{ color: "red" }}>{error}</p>}
        {isRecording && !isUploading && !error && (
          <p>ë§ì”€í•˜ì‹  í›„ í™”ë©´ì„ ëˆŒëŸ¬ ë…¹ìŒì„ ë§ˆì¹˜ê³  ì „ì†¡í•´ ì£¼ì„¸ìš”.</p>
        )}
        {isUploading && <p>ì¸ì‹ ì¤‘ì…ë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ ì£¼ì„¸ìš”...</p>}
        {sttResult && !isUploading && (
          <>
            <p style={{ fontWeight: "bold", marginBottom: 8 }}>ì¸ì‹ëœ í…ìŠ¤íŠ¸</p>
            <p>{sttResult}</p>
          </>
        )}

        {ttsUrl && !isUploading && (
          <div style={{ marginTop: 16 }}>
            <p style={{ fontWeight: "bold", marginBottom: 4 }}>ì•ˆë‚´ ìŒì„±</p>
            <audio src={ttsUrl} controls autoPlay />
          </div>
        )}
      </div>
    </Layout>
  );
}

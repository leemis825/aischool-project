import { useNavigate } from "react-router-dom";
import Layout from "../components/Layout.js";
import { useEffect, useRef, useState } from "react";
// ì–¸ë‹ˆ ì—¬ê¸°ì˜ˆìš”1ğŸ¦ŠğŸ°
import { sttAndMinwon, type SttMinwonResponse } from "../services/sttService";
import { requestTts } from "../services/ttsService";
import SpeakerImg from "../assets/speaker.png";

export default function ListeningPage() {
  const navigate = useNavigate();

  const [isRecording, setIsRecording] = useState(false);
  const [isUploading, setIsUploading] = useState(false);
  const [error, setError] = useState<string | null>(null);
  const [sttResult, setSttResult] = useState<string>("");
  const [ttsUrl, setTtsUrl] = useState<string | null>(null); // ğŸ”¹ TTS ì˜¤ë””ì˜¤ URL
  const [volume, setVolume] = useState(0);
  const [sessionId, setSessionId] = useState<string | null>(null); // ğŸ”¹ ë°±ì—”ë“œ ì„¸ì…˜ ID
  const sessionIdRef = useRef<string | null>(null);

  // ğŸ”¹ ë…¹ìŒ ê´€ë ¨ refë“¤
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const mediaStreamRef = useRef<MediaStream | null>(null);
  const chunksRef = useRef<Blob[]>([]);

  // ğŸ”¹ ì˜¤ë””ì˜¤ ë¹„ì£¼ì–¼ë¼ì´ì €ìš© refë“¤
  const audioContextRef = useRef<AudioContext | null>(null);
  const analyserRef = useRef<AnalyserNode | null>(null);
  const animationFrameRef = useRef<number | null>(null);
  const canvasRef = useRef<HTMLCanvasElement | null>(null);

  const trackVolume = () => {
    const analyser = analyserRef.current;
    if (!analyser) return;

    const bufferLength = analyser.fftSize;
    const dataArray = new Uint8Array(bufferLength);

    const update = () => {
      animationFrameRef.current = requestAnimationFrame(update);

      analyser.getByteTimeDomainData(dataArray);

      let sum = 0;
      for (let i = 0; i < bufferLength; i++) {
        const v = (dataArray[i] ?? 128) - 128;
        sum += Math.abs(v);
      }
      const avg = sum / bufferLength; // 0~128
      const normalized = Math.min(avg / 64, 1); // 0~1 ë²”ìœ„ë¡œ ì •ê·œí™”

      setVolume(normalized);
    };

    update();
  };

  const stopVisualizer = () => {
    // ì• ë‹ˆë©”ì´ì…˜ ë£¨í”„ ì¤‘ì§€
    if (animationFrameRef.current !== null) {
      cancelAnimationFrame(animationFrameRef.current);
      animationFrameRef.current = null;
    }

    // ìº”ë²„ìŠ¤ ê¹¨ë—í•˜ê²Œ ì§€ìš°ê³  ë°°ê²½ë§Œ ì±„ìš°ê¸°
    const canvas = canvasRef.current;
    if (!canvas) return;

    const ctx = canvas.getContext("2d");
    if (!ctx) return;

    const { width, height } = canvas;
    ctx.clearRect(0, 0, width, height);
    ctx.fillStyle = "#4E9948";
    ctx.fillRect(0, 0, width, height);
  };

  // ğŸ”¹ ë…¹ìŒ + ë¹„ì£¼ì–¼ë¼ì´ì € ì„¸íŒ…ì„ í•¨ìˆ˜ë¡œ ë¶„ë¦¬ (ì´ˆê¸° + clarification ì´í›„ ì¬ì‚¬ìš©)
  const setupRecorderAndVisualizer = async () => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: true,
      });
      mediaStreamRef.current = stream;

      /**
       * 1) MediaRecorder ì„¤ì • (ë…¹ìŒìš©)
       */
      const options: MediaRecorderOptions = {};
      if (MediaRecorder.isTypeSupported("audio/webm;codecs=opus")) {
        options.mimeType = "audio/webm;codecs=opus";
      } else if (MediaRecorder.isTypeSupported("audio/webm")) {
        options.mimeType = "audio/webm";
      }

      const recorder = new MediaRecorder(stream, options);

      recorder.ondataavailable = (event: BlobEvent) => {
        if (event.data && event.data.size > 0) {
          chunksRef.current.push(event.data);
        }
      };

      recorder.onstop = async () => {
        try {
          const blob = new Blob(chunksRef.current, { type: "audio/webm" });
          chunksRef.current = [];

          await uploadBlob(blob);
        } catch (err) {
          console.error(err);
          setError("ë…¹ìŒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆì–´ìš”. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.");
          setIsUploading(false);
        }
      };

      mediaRecorderRef.current = recorder;

      /**
       * 2) AudioContext + Analyser ì„¤ì • (íŒŒí˜• ê·¸ë¦¬ê¸°ìš©)
       */
      const audioCtx = new (window.AudioContext ||
        (window as any).webkitAudioContext)();
      audioContextRef.current = audioCtx;

      const source = audioCtx.createMediaStreamSource(stream);
      const analyser = audioCtx.createAnalyser();
      analyser.fftSize = 2048; // í•´ìƒë„
      source.connect(analyser);
      analyserRef.current = analyser;

      if (analyserRef.current) {
        trackVolume();
      }

      // ìë™ ë…¹ìŒ ì‹œì‘
      recorder.start();
      setIsRecording(true);
      setIsUploading(false);
      setError(null);
    } catch (e) {
      console.error(e);
      setError("ë§ˆì´í¬ ê¶Œí•œì„ í—ˆìš©í•´ ì£¼ì„¸ìš”.");
    }
  };

  useEffect(() => {
    // ì²« ì§„ì… ì‹œ í•œ ë²ˆë§Œ ë…¹ìŒ ì‹œì‘
    setupRecorderAndVisualizer();

    // ì–¸ë§ˆìš´íŠ¸ ì‹œ ì •ë¦¬
    return () => {
      try {
        if (
          mediaRecorderRef.current &&
          mediaRecorderRef.current.state !== "inactive"
        ) {
          mediaRecorderRef.current.stop();
        }
      } catch {
        // ignore
      }

      if (mediaStreamRef.current) {
        mediaStreamRef.current.getTracks().forEach((t) => t.stop());
      }

      if (animationFrameRef.current !== null) {
        cancelAnimationFrame(animationFrameRef.current);
      }

      if (audioContextRef.current) {
        audioContextRef.current.close();
      }

      stopVisualizer();
    };
    // eslint-disable-next-line react-hooks/exhaustive-deps
  }, []);

  // ì–¸ë‹ˆ ì—¬ê¸°ì˜ˆìš”2ğŸ¦ŠğŸ°
  const callTTS = async (text: string) => {
    try {
      const trimmed = text?.trim();
      if (!trimmed) return;

      // ì´ì „ì— ë§Œë“  ì˜¤ë””ì˜¤ URLì´ ìˆìœ¼ë©´ ì •ë¦¬
      if (ttsUrl) {
        URL.revokeObjectURL(ttsUrl);
      }

      const blob = await requestTts(trimmed); // â† ì„œë¹„ìŠ¤ í•¨ìˆ˜ í˜¸ì¶œ
      const url = URL.createObjectURL(blob);
      setTtsUrl(url);
    } catch (e) {
      console.error(e);
      setError("ì•ˆë‚´ ìŒì„±ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆì–´ìš”.");
    }
  };

  const uploadBlob = async (blob: Blob) => {
    setError(null);

    try {
      console.log("ğŸ¤ì „ì†¡í•  ì˜¤ë””ì˜¤ Blob:", blob);
      console.log(
        "ğŸ‘‰ uploadBlob ì§ì „ sessionIdRef.current =",
        sessionIdRef.current
      );

      const file = new File([blob], "voice.webm", { type: "audio/webm" });

      // ğŸ”¥ ì´ë¯¸ ì„¸ì…˜ IDê°€ ìˆìœ¼ë©´ ê·¸ê±¸ ê³„ì† ì‚¬ìš©
      const result: SttMinwonResponse = await sttAndMinwon(
        file,
        sessionIdRef.current
      );
      console.log("ğŸ”Š STT+ë¯¼ì› ì—”ì§„ ê²°ê³¼:", result);

      const finalText = result.text || "(ë¹ˆ í…ìŠ¤íŠ¸)";
      setSttResult(finalText);
      setIsUploading(false);

      // âœ… ì„¸ì…˜ IDëŠ” "ì²˜ìŒ í•œ ë²ˆë§Œ" ì„¸íŒ…
      if (!sessionIdRef.current && result.session_id) {
        sessionIdRef.current = result.session_id;
        setSessionId(result.session_id);
        console.log("âœ… ì„¸ì…˜ ID ìµœì´ˆ ì„¤ì •:", result.session_id);
      } else if (
        sessionIdRef.current &&
        result.session_id &&
        result.session_id !== sessionIdRef.current
      ) {
        console.warn(
          "âš ï¸ ì„œë²„ê°€ ë‹¤ë¥¸ session_idë¥¼ ëŒë ¤ì¤¬ì–´ìš”. ê¸°ì¡´ ê²ƒì„ ìœ ì§€í•©ë‹ˆë‹¤.",
          {
            current: sessionIdRef.current,
            returned: result.session_id,
          }
        );
        // ì—¬ê¸°ì„œëŠ” ê·¸ëƒ¥ ë¬´ì‹œí•˜ê³  ê¸°ì¡´ sessionIdRef.currentë¥¼ ê³„ì† ì‚¬ìš©
      }

      await callTTS(
        result.user_facing?.main_message ??
          "ë§ì”€í•´ ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ ì£¼ì„¸ìš”."
      );

      const stage = result.engine_result?.stage;

      if (stage === "clarification") {
        console.log("ğŸ” clarification ë‹¨ê³„ â€“ ë‹¤ì‹œ ë…¹ìŒ ëŒ€ê¸°");
        await setupRecorderAndVisualizer();
        return;
      }

      navigate("/summary", {
        state: {
          sttText: finalText,
          summary: result.staff_payload?.summary,
          engineResult: result.engine_result,
        },
      });
    } catch (e) {
      console.error(e);
      setError("ë…¹ìŒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆì–´ìš”. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.");
      setIsUploading(false);
    }
  };

  // ğŸ”¹ í™”ë©´ íƒ­ ì‹œ: ë…¹ìŒ ì¢…ë£Œ + ì—…ë¡œë“œ
  const handleClick = () => {
    if (!isRecording || isUploading) return;
    if (!mediaRecorderRef.current) {
      setError("ë…¹ìŒê¸°ê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ì–´ìš”.");
      return;
    }

    setIsUploading(true);

    try {
      mediaRecorderRef.current.stop();
      setIsRecording(false);

      // ğŸ”¥ ì—¬ê¸°ì„œ íŒŒí˜• ë„ê¸°
      stopVisualizer();
    } catch (e) {
      console.error(e);
      setError("ë…¹ìŒì„ ì¤‘ë‹¨í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆì–´ìš”.");
      setIsUploading(false);
    }
  };

  return (
    <Layout
      title="ë¯¼ì›ì ‘ìˆ˜"
      content="ë§ì”€ì„ ë“£ê³  ìˆì–´ìš”"
      topImage="src/assets/top2.png"
      onClick={handleClick}
    >
      <div
        style={{
          display: "flex",
          justifyContent: "center",
          alignItems: "center",
          flexDirection: "column",
          marginTop: "25px",
        }}
      >
        <img
          src={SpeakerImg}
          alt="speaker"
          style={{
            width: "230px",
            height: "230px",
            marginTop: "-50px",
            marginBottom: "20px",
            transition: "transform 0.05s linear",
            transform:
              isRecording && !isUploading
                ? `scale(${1 + Math.sin(volume * 10) * 0.2})` // ğŸ”Š ë…¹ìŒ ì¤‘ì—ë§Œ ê¿ˆí‹€
                : "scale(1)", // ğŸ”‡ ì•„ë‹ˆë©´ ê³ ì •
          }}
        />

        {error && <p style={{ color: "red" }}>{error}</p>}
        {isRecording && !isUploading && !error && (
          <h2>ë§ì”€ì´ ëë‚˜ë©´ í™”ë©´ ì–´ë””ë“  ëˆŒëŸ¬ì£¼ì„¸ìš”</h2>
        )}
        {isUploading && <h2>ì¸ì‹ ì¤‘ì…ë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ ì£¼ì„¸ìš”...</h2>}
        {sttResult && !isUploading}

        {ttsUrl && !isUploading && (
          <div style={{ marginTop: 16 }}>
            <audio src={ttsUrl} controls autoPlay />
          </div>
        )}
      </div>
    </Layout>
  );
}
